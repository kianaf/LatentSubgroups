{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for using variational autoencoder and the choice of variables in struct depends on which version you want to train. \n",
    "\n",
    "- with Pre-transformations\n",
    "- with AIQN\n",
    "- standard VAE\n",
    "\n",
    "The results in the paper have the following architecture for the sim data:\n",
    "\n",
    "    data_string::String = \"sim\" \n",
    "    η::Float32 = 1e-3                                                                                                \n",
    "    λ::Float32 = 0.01f0\n",
    "    β::Float64 = 0.5                                                                                      \n",
    "    batch_size::Int = 128                                                                                            \n",
    "    epochs::Int = 500                                                                                                 \n",
    "    seed::Int = 42                                                                                                  \n",
    "    input_dim::Int = 21                                                                                               \n",
    "    latent_dim::Int = 2                                                                                               \n",
    "    hidden_dim::Int = 28                                                                                              \n",
    "    verbose_freq::Int = 100                                                                                          \n",
    "    hyperopt_flag::Bool = false       \n",
    "    multimodal_encoder::Bool = true   \n",
    "\n",
    "    pre_transformation::Bool =  true                                                                                 \n",
    "    bimodality_score_threshold::Float32 = 0                                                                         \n",
    "    \n",
    "    scaling::Bool = true                                                                                             \n",
    "    scaling_method::String = \"scaling\" \n",
    "                                                                                    \n",
    "    AIQN::Bool = false                                                                                               \n",
    "                                                                                 \n",
    "    synthetic_data::Bool = false                                                                                     \n",
    "\n",
    "\n",
    "Now for QVAE:\n",
    "    AIQN = true\n",
    "\n",
    "for standard VAE:\n",
    "    pre_transformation = false\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The results in the paper have the following architecture for the IST data:\n",
    "\n",
    "    data_string::String = \"ist_randomization_data_smaller_no_west_no_south_aug5\" \n",
    "    η::Float32 = 5e-4                                                                                                \n",
    "    λ::Float32 = 0.01f0\n",
    "    β::Float64 = 0.5                                                                                      \n",
    "    batch_size::Int = 32                                                                                            \n",
    "    epochs::Int = 1000                                                                                                 \n",
    "    seed::Int = 42                                                                                                  \n",
    "    input_dim::Int = 21                                                                                               \n",
    "    latent_dim::Int = 2                                                                                               \n",
    "    hidden_dim::Int = 28                                                                                              \n",
    "    verbose_freq::Int = 100                                                                                          \n",
    "    hyperopt_flag::Bool = false       \n",
    "    multimodal_encoder::Bool = true   \n",
    "\n",
    "    pre_transformation::Bool =  true                                                                                 \n",
    "    bimodality_score_threshold::Float32 = 0                                                                         \n",
    "    \n",
    "    scaling::Bool = true                                                                                             \n",
    "    scaling_method::String = \"scaling\" \n",
    "                                                                                    \n",
    "    AIQN::Bool = false                                                                                               \n",
    "                                                                                 \n",
    "    synthetic_data::Bool = false                                                                                     \n",
    "\n",
    "\n",
    "Now for QVAE:\n",
    "    AIQN = true\n",
    "\n",
    "for standard VAE:\n",
    "    pre_transformation = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/farhadyar/Documents/Project_PTVAE/progs/github_repo/LatentSubgroups\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cd(\"../.\") \n",
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/Documents/Project_PTVAE/progs/github_repo/LatentSubgroups/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "if isfile(\"Project.toml\") && isfile(\"Manifest.toml\")\n",
    "    Pkg.activate(\".\")\n",
    "end\n",
    "\n",
    "# Pkg.instantiate()\n",
    "using IJulia\n",
    "\n",
    "using Revise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using DecisionTree.predict in module Main conflicts with an existing identifier.\n"
     ]
    }
   ],
   "source": [
    "includet(\"../AIQN/AIQN.jl\")\n",
    "includet(\"../src/structs.jl\")\n",
    "includet(\"../src/report.jl\")\n",
    "includet(\"../src/transformations.jl\")\n",
    "includet(\"../src/VAE.jl\")\n",
    "includet(\"../src/load_data.jl\")\n",
    "includet(\"../src/evaluation/evaluation.jl\")\n",
    "includet(\"../src/classification.jl\")\n",
    "includet(\"../src/GLM.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, dataTypeArray,args = load_dataset()\n",
    "\n",
    "args.cross_validation_flag = true\n",
    "args.n_folds = 10\n",
    "\n",
    "\n",
    "if args.data_string == \"sim\"\n",
    "    \n",
    "    args.β = 0.5\n",
    "    args.η = 1e-3\n",
    "    args.epochs = 500\n",
    "    args.multimodal_encoder = true\n",
    "    args.batch_size = 32\n",
    "    args.latent_dim = 2\n",
    "    args.hidden_dim = 28\n",
    "    args.scaling_method = \"scaling\"\n",
    "    args.IPW_sampling = false\n",
    "\n",
    "else contains(args.data_string, \"ist\")\n",
    "    args.β = 0.5\n",
    "    args.η = 1e-3\n",
    "    args.epochs = 1000\n",
    "    args.multimodal_encoder = true\n",
    "    args.batch_size = 128\n",
    "    args.latent_dim = 2\n",
    "    args.hidden_dim = 22\n",
    "    args.scaling_method = \"scaling\"\n",
    "end\n",
    "\n",
    "\n",
    "args.pre_transformation = true\n",
    "\n",
    "args.AIQN = false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing (transformations and scaling) &\n",
    "## Training Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: loss = 2.773278758823861\n",
      "Epoch 200: loss = 2.9145162232575546\n",
      "Epoch 300: loss = 2.601037079384966\n",
      "Epoch 400: loss = 2.580703933500228\n",
      "Epoch 500: loss = 2.3359099957898017\n",
      "Epoch 100: loss = 2.846485695957449\n",
      "Epoch 200: loss = 2.59240343464286\n",
      "Epoch 300: loss = 2.4469880291882053\n",
      "Epoch 400: loss = 2.531275340588725\n",
      "Epoch 500: loss = 2.4596551211982165\n",
      "Epoch 100: loss = 2.864328112147001\n",
      "Epoch 200: loss = 2.5672353660600664\n",
      "Epoch 300: loss = 2.4417963462688115\n",
      "Epoch 400: loss = 2.5057129072398725\n",
      "Epoch 500: loss = 2.5932919125913676\n",
      "Epoch 100: loss = 2.7321363565540557\n",
      "Epoch 200: loss = 2.533614436055833\n",
      "Epoch 300: loss = 2.382207664820421\n",
      "Epoch 400: loss = 2.403969955562805\n",
      "Epoch 500: loss = 2.394177447097772\n",
      "Epoch 100: loss = 2.7279418592079434\n",
      "Epoch 200: loss = 2.445441486233846\n",
      "Epoch 300: loss = 2.4201817574357105\n",
      "Epoch 400: loss = 2.3171895708208488\n",
      "Epoch 500: loss = 2.401005751915347\n",
      "Epoch 100: loss = 2.719563659968318\n",
      "Epoch 200: loss = 2.487140405006585\n",
      "Epoch 300: loss = 2.707346383169527\n",
      "Epoch 400: loss = 2.4939508227333302\n",
      "Epoch 500: loss = 2.607063736282168\n",
      "Epoch 100: loss = 2.730868473933157\n",
      "Epoch 200: loss = 2.5522525146842128\n",
      "Epoch 300: loss = 2.8436519563212124\n",
      "Epoch 400: loss = 2.4570474147637063\n",
      "Epoch 500: loss = 2.3906736176121064\n",
      "Epoch 100: loss = 2.8340801463433847\n",
      "Epoch 200: loss = 2.570781779488646\n",
      "Epoch 300: loss = 2.7259478385408755\n",
      "Epoch 400: loss = 2.417245571905468\n",
      "Epoch 500: loss = 2.586654042594845\n",
      "Epoch 100: loss = 2.925354403151191\n",
      "Epoch 200: loss = 2.6541152217365966\n",
      "Epoch 300: loss = 2.6952433665070394\n",
      "Epoch 400: loss = 2.4251618705231435\n",
      "Epoch 500: loss = 2.311033405989047\n",
      "Epoch 100: loss = 3.0360713250358615\n",
      "Epoch 200: loss = 2.85692037998789\n",
      "Epoch 300: loss = 2.5006154326343717\n",
      "Epoch 400: loss = 2.3823311959036753\n",
      "Epoch 500: loss = 2.284506821386765\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(11)\n",
    "\n",
    "if args.cross_validation_flag\n",
    "\n",
    "    reconstruction_train_val_sets = []\n",
    "\n",
    "    cross_val_run_path = string(args.current_run_path, \"/$(args.n_folds)fold_cross_validation\")\n",
    "    mkdir(cross_val_run_path)\n",
    "\n",
    "    cross_val_sets = create_cross_validation_sets(x, args.n_folds)\n",
    "\n",
    "    for fold = 1:args.n_folds    \n",
    "\n",
    "        train_set, val_set = cross_val_sets[fold]\n",
    "               \n",
    "        args.current_run_path = string(cross_val_run_path, \"/fold_$(fold)\")\n",
    "\n",
    "\n",
    "        mkdir(args.current_run_path)\n",
    "\n",
    "        # write the train_set and val_set as csv in args.current_run_path\n",
    "        writedlm(string(args.current_run_path, \"/\", \"train.csv\"),  train_set, ',')\n",
    "        writedlm(string(args.current_run_path, \"/\", \"val.csv\"),  val_set, ',')\n",
    "            \n",
    "        preprocess_ps = preprocess_params(input_dim = args.input_dim, pre_transformation_type = \"quantile\")\n",
    "        preprocessed_data, preprocess_ps = preprocess!(args, preprocess_ps, train_set, dataTypeArray)\n",
    "        preprocessed_data_val, preprocess_ps = preprocess_test_data(args, preprocess_ps, val_set, dataTypeArray)\n",
    "\n",
    "        if args.hyperopt_flag \n",
    "            println(\"cross_validation and hyper parameter optimization is not implemented!\")\n",
    "            println(\"hyperopt_flag is changed to false\")\n",
    "        else\n",
    "            val_data = get_data(preprocessed_data_val, args.batch_size)\n",
    "\n",
    "            model, training_data, reconstruction_train_val_set = trainVAE!(preprocessed_data, train_set, dataTypeArray, preprocess_ps, args; val_data = val_data)\n",
    "\n",
    "            push!(reconstruction_train_val_sets, reconstruction_train_val_set)\n",
    "\n",
    "            \n",
    "            save_vae_results(val_data, preprocessed_data, val_set, model, preprocess_ps, args, [], true)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    mkdir(string(cross_val_run_path, \"/reconstruction_values\"))\n",
    "\n",
    "    @save string(cross_val_run_path, \"/reconstruction_values/reconstruction_values\") reconstruction_train_val_sets\n",
    "\n",
    "else\n",
    "    preprocess_ps = preprocess_params(input_dim = args.input_dim)\n",
    "    preprocessed_data, preprocess_ps = preprocess!(args, preprocess_ps, x, dataTypeArray)\n",
    "\n",
    "    if args.hyperopt_flag\n",
    "        trainVAE_hyperparams_opt!(preprocessed_data, x, dataTypeArray, preprocess_ps, args)\n",
    "    else\n",
    "        model, training_data, loss_array_vae = trainVAE!(preprocessed_data, x, dataTypeArray, preprocess_ps, args)\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.7",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbe963f0ca08ef66471a02bca175dbdba8b50cc2ceaf0dd1c5a92a4fd1518261"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
